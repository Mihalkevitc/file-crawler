# Инструкция по запуску проекта

## Описание проекта

Этот проект представляет собой поисковую систему для документов. Система позволяет сканировать папку с файлами, извлекать из них текст и загружать в базу данных с полнотекстовым поиском. Поддерживаются форматы: doc, docx, xls, xlsx, pdf, txt, а также архивы zip, rar, 7z с вложенными файлами.

## Состав проекта

- generate_files.py - генератор тестовых файлов
- crawler.py - краулер для сбора данных
- import_to_db.py - загрузка в БД и поиск
- requirements.txt - список зависимостей

## Требования

- Python 3.7 или выше
- pip (менеджер пакетов Python)
- Git (для клонирования репозитория)

## Установка

### 1. Клонирование репозитория

Откройте терминал и выполните команду:

```bash
git clone https://github.com/Mihalkevitc/file-crawler.git
cd file-crawler
```

### 2. Создание виртуального окружения (рекомендуется)

Для Windows:

```bash
python -m venv venv
source venv/Scripts/activate
```

Для Linux/Mac:

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Установка зависимостей

```bash
pip install -r requirements.txt
```

## Использование

### 1. Генерация тестовых файлов

Эта команда создаст тестовые файлы в папке data/:

```bash
python generate_files.py
```

Будут созданы:
- 3 файла каждого типа (doc, docx, xls, xlsx, pdf, txt) в папке data/docs/
- по одному архиву каждого типа (zip, rar, 7z) в папке data/archives/

### 2. Запуск краулера

Краулер просканирует все файлы, извлечёт из них текст и сохранит результаты в CSV:

```bash
python crawler.py
```

Результат будет сохранён в файл output/file_index.csv

### 3. Загрузка в базу данных и поиск

Эта команда создаст базу SQLite с полнотекстовым поиском и выполнит тестовые запросы:

```bash
python import_to_db.py
```

## Структура проекта после запуска

```
.
├── data/
│   ├── docs/          # сгенерированные документы
│   └── archives/      # сгенерированные архивы
├── output/
│   └── file_index.csv # результаты краулера
├── database/
│   └── files.db       # база данных SQLite
├── generate_files.py
├── crawler.py
├── import_to_db.py
└── requirements.txt
```

## Поиск в базе данных

После импорта можно искать по словам. Функция search принимает поисковый запрос и возвращает список файлов, где это слово встречается, с подсветкой результатов.

Пример использования в коде:

```python
from import_to_db import search

results = search('инвестиции')
for file_name, file_path, snippet in results:
    print(f"Файл: {file_name}")
    print(f"Путь: {file_path}")
    print(f"Фрагмент: {snippet}")
    print()
```

## Примечания

- Для работы с rar архивами необходима установка утилиты unrar (в Windows обычно ставится вместе с WinRAR)
- Для корректной работы с pdf убедитесь, что файлы содержат текстовый слой (не отсканированные изображения)
- При первом запуске generate_files.py может занять некоторое время на генерацию данных
- База данных создаётся заново при каждом импорте
